#####################################
##
## What: MPSprofiling.R
## Who : Frank RÃ¼hle
## When: 29-01-2020
##
## Processing barcode count data and calculation of protein stability indices (PSIs)
##
## Args:
## -----
## targets=targets.txt      # file describing the targets.
## prefix=RE                # prefix to remove from the sample name
## suffix=RE                # suffix to remove from the sample name (usually _readcounts.tsv)
## inputdir=.               # input directory where the files .tsv files are located
## out="MPSprofiling"       # output directory
## expdesign="amplicon1     # experiment design
##
######################################
options(stringsAsFactors=FALSE)
library(reshape)
library(kableExtra)
library(plyr)
library(dplyr)
library(tidyr)
library(Biobase)
library(Biostrings)
library(mixtools)
library(RColorBrewer)
library(gplots)
library(ggplot2)
library(ggbeeswarm)
library(ggrepel)
library(pheatmap)

##
## get arguments from the command line
##
parseArgs <- function(args,string,default=NULL,convert="as.character") {

    if(length(i <- grep(string,args,fixed=T)) == 1)
        return(do.call(convert,list(gsub(string,"",args[i]))))
    
    if(!is.null(default)) default else do.call(convert,list(NA))
}

args <- commandArgs(T)
ftargets     <- parseArgs(args,"targets=","targets.txt")     # file describing the targets
pre          <- parseArgs(args,"prefix=","")    # prefix to remove from the sample name
suf          <- parseArgs(args,"suffix=","")    # suffix to remove from the sample name
inputdir     <- parseArgs(args,"inputdir=","./")     # input files directory
out          <- parseArgs(args,"out=","MPSprofiling") # output directory
expdesign  <- parseArgs(args,"expdesign=","amplicon1")  

runstr <- "Rscript MPSprofiling.R [targets=targets.txt] [prefix=RE] [suffix=RE] [inputdir=.] [out=MPSprofiling] [expdesign=amplicon1]"
if(!file.exists(ftargets))   stop(paste("File",ftargets,"does NOT exist. Run with:\n",runstr))
if(!file.exists(inputdir))   stop(paste("Dir",inputdir,"does NOT exist. Run with:\n",runstr))



### columns in target file
# experiment: overall experiment name
# sub_experiment: summarizes those samples which have been distributed to stability bins and belong together for PSI calculation (can be empty if same as experiment)
# unique_sample_id: sample identifier. If no demultiplexing necessary, may be same as pruned_file_name
# pruned_file_name: input file names after removing common prefixes and suffixes. Is grebbed against count file name to merge targets.txt to the count data.
# fraction
# bin
# barcode_demultiplex (if design "amplicon3" or "amplicon4") for demultiplexing samples from count file 


# set parameters
required_target_columns <- c("pruned_file_name", "unique_sample_id", "experiment", "sub_experiment", "bin", "fraction")
need2demultiplex <- expdesign %in% c("amplicon3", "amplicon4")
if(need2demultiplex) {required_target_columns <- c(required_target_columns, "barcode_demultiplex")}
annoFactors <- c("experiment", "sub_experiment")
threshold_rel_countssum <- 0.001 # threshold for first qc filtering (proportion of sample sum from mean sample sum)
applyThreshBeforeBGsubt =FALSE # for background subtraction: apply cutoff threshold before subtracting background value (not done in CombinatorialProfiler)
removeZerosAfterBGsubt = FALSE # remove zero counts generated by background subtraction (not done in CombinatorialProfiler)   

# create output folders
if(!dir.exists(out)) {dir.create(out, recursive = T)}
name_plotfolder <- file.path(out, "plots")
if(!dir.exists(name_plotfolder)) {dir.create(name_plotfolder, recursive = T)}



# read targets file
targets <- read.delim(ftargets, header=T, comment.char="#", sep=",")
if (!all(required_target_columns %in% colnames(targets))) {
  missing_cols <- paste(required_target_columns[! required_target_columns %in% colnames(targets)], collapse=", ")
  stop("\n\nMissung required target column(s):", missing_cols)
}
targets <- targets[,required_target_columns]

# read count files
  f <- list.files(paste0(inputdir), pattern="\\.barcode_count\\.tsv$", full.names=TRUE) 
  f <- f[!grepl("Undetermined", f)] # don't use count file with unmapped reads if present
  
  counts <- mclapply(f, read.delim, header=T, comment.char = "#")
  names(counts) <- basename(f)
  
 
  # merge counts with targets file (includes optional demultiplexing)
  counts <- lapply(names(counts), function(x) {
    colnames(counts[[x]]) <- c("sequence", "id", "count")
    
    if(need2demultiplex) { 
        # if true, counts[[x] contains multiple samples in one file instead of just 1 sample. 
        # id is 2nd extracted barcode and must correspond to "barcode_demultiplex" in targets.txt.
        logindex_targetsfile <- sapply(targets$pruned_file_name, grepl, x)
        if(sum(logindex_targetsfile) != length(unique(counts[[x]]$id))) {
          stop("\npruned_file_name in targets.txt does not correspond to the correct number of ids in count file ", x)}
        merge(counts[[x]], targets[logindex_targetsfile,], all.x=T, by.x="id", by.y="barcode_demultiplex")
    } else { 
        # if false, counts[[x] contains a single sample. 
        # id is sample file name including bpipe suffixes (not needed since count file name is used)
        logindex_targetsfile <- sapply(targets$pruned_file_name, grepl, x)
        if(sum(logindex_targetsfile) !=1) {stop("\npruned_file_name in targets.txt not unique for count file name ", x)}
        counts[[x]] <- data.frame(counts[[x]][, c("sequence", "count")], targets[logindex_targetsfile,])
    }
  })

  counts <- rbind.fill(counts) # rbind list elements to data.frame
  
  # remove barcodes containg Ns
  counts <- counts[!grepl("N", counts$sequence), ]
    
  
  # sum all counts per sample for initial QC filtering
  counts_summary <- counts %>%
    dplyr::group_by(unique_sample_id) %>%
    dplyr::summarize(sum = sum(count)) %>%
    dplyr::ungroup() 
    counts <- merge(counts, counts_summary, by="unique_sample_id", all.x=T)
      
      # 1st filter step for extreme outlier (need counts_summary for correct mean(counts_summary$sum))
      removedSamples <- as.data.frame(counts_summary[counts_summary$sum < threshold_rel_countssum * mean(counts_summary$sum), c("unique_sample_id", "sum")])
      cat("\nremove", nrow(removedSamples), "samples from dataset with <", 100*threshold_rel_countssum, "% of mean total counts:\n")
      print(removedSamples)
      counts <- filter(counts, !(sum < threshold_rel_countssum * mean(counts_summary$sum)))
      
      
  
    #### start analsis per experiment
    # initalise result lists for all experiments
     counts_all <- list()
     bynuc_all <- list()
     byaa_all <- list()
     
     allExpCounts <- counts
    for (e in unique(allExpCounts$experiment)) { # this loop may be obsolet
      counts <- allExpCounts[allExpCounts$experiment == e, ]

      # factorize categorical variables after subsetting and initial outlier removal
      category_vars <- colnames(counts)[colnames(counts) %in% c("unique_sample_id", "experiment", "sub_experiment", "bin", "sequence")]
      counts[,category_vars] <- lapply(counts[,category_vars], factor)
      
      
      # decide whether column "sub_experiment" is used (e.g. for strain names) 
      use_sub_experiment <- !( !"sub_experiment" %in% colnames(counts) || any(is.na(counts$sub_experiment)) || 
                                 any(counts$sub_experiment=="") || all(as.character(counts$sub_experiment) == as.character(counts$experiment)))
      if(use_sub_experiment) {counts$helper_cat <- factor(paste(counts$experiment, counts$sub_experiment, sep="_")) # as long as in e loop, could use sub_experiment only
                            } else {counts$helper_cat <- factor(counts$experiment)}
      
    
    # add columns bin_rank per (sub)experiment and column totalbins
      counts_summary2 <- unique(counts[,colnames(counts) %in% c("unique_sample_id", "experiment", "sub_experiment", "bin", "helper_cat")]) %>%
      dplyr::group_by(helper_cat)  %>%
      dplyr::mutate(bin_rank=rank(bin)) %>% # summarize does not work for bin_rank
      dplyr::mutate(totalbins = dplyr::n_distinct(bin)) %>% 
      dplyr::ungroup() 

    counts_summary2 <- as.data.frame(counts_summary2)
    counts <- merge(counts, counts_summary2[,c("unique_sample_id", "bin_rank", "totalbins")], by="unique_sample_id", all.x=T)
    
    # add column f as transformed bin index [0,1]
    counts$f <- (counts$bin_rank -1) / (counts$totalbins -1) 
  
    
  # in case there are sequences with zero counts, these entries are removed
  removeZerosRaw = FALSE # there are no zero counts since only observed sequences are counted
  if(removeZerosRaw) {
    logindex_nocounts <- counts$count==0
    if(sum(logindex_nocounts)>0) {
      cat("\n", sum(logindex_nocounts), "entries removed from raw count matrix due to zero counts:\n")
      print(counts[logindex_nocounts, c("sample_name", "bin", "sequence")][1:min(20,sum(logindex_nocounts)),])
      cat("\n")
      counts <- counts[!logindex_nocounts,]
     }
    }
  
  

  ### background subtraction
  # and run downstream calculations separated for with and without background subtraction
  
  # To try to remove the noise, we will try to fit a Gaussian mixture model to each sample and subtract the mean of the background distribution. In addition,
  # we will threshold the raw counts to remove the background distribution. There are several cases to deal with: The case of well-separable distributions,
  # like wild type fraction E, the case of non-separable distributions like ubr1-mak3 fraction H, and the case where the background distribution completely
  # dominates, like ubr1-ufd4 fraction B. The threshold depends on the shape and position of the distributions. If the 97.5th percentile of the background
  # distribution is larger than the 2.5th percentile of the foreground distribution and the mean of the background distribution is lower than the mean of
  # the foreground distribution, we set the threshold at the 2.5th percentile of the foreground distribution to include all foreground reads, since this case
  # corresponds to non-separable distribution. Otherwise we threshold at the 97.5th percentile of the background distribution, which applies to well-separable
  # cases and cases with completely dominant background distribution.
  # options(try.outFile = stdout()) 
  
  set.seed(100)
  
  bgsubt <- try(counts %>% 
    group_by(helper_cat, bin) %>% # determine bg and threshold per bin
    do({
      ret <- .
      mix <- normalmixEM(log10(ret$count), maxrestarts=500)
      ubounds <- 10^qnorm(0.975, mix$mu, mix$sigma)
      lbounds <- 10^qnorm(0.025, mix$mu, mix$sigma)
      bg_comp <- which.min(ubounds)
      if (ubounds[bg_comp] > lbounds[-bg_comp] && mix$mu[bg_comp] < mix$mu[-bg_comp]) {
        thresh <- lbounds[-bg_comp]
      } #else if (ubounds[bg_comp] > lbounds[-bg_comp] && mix$mu[bg_comp] < mix$mu[-bg_comp])
      else {
        thresh <- ubounds[bg_comp]
      }
      cat(paste("experiment:", unique(ret$helper_cat), "; bin:", unique(ret$bin), if(applyThreshBeforeBGsubt) {paste("; threshold applied:", round(thresh, 3))} else {""}, "; bg subtracted:", round(10^mix$mu[bg_comp], 3), "\n")) 
      if(applyThreshBeforeBGsubt) {ret$count <- ifelse(ret$count > thresh, ret$count, 0)} # commented out
      ret$count_bgsubt <- pmax(0, ret$count - 10^mix$mu[bg_comp])
      ret$count <- ret$count_bgsubt
      ret$count_bgsubt <- NULL
      as.data.frame(ret)
    }) )

   
   if ("try-error" %in% class(bgsubt)) {
     cat("\nFailed to distinguish background by fitting a Gaussian mixture model. Background subtraction omitted!\n\n")
     bgloop <- c("raw")
   } else {
     bgloop <- c("raw", "bgsubt")
      # in case there are sequences with zero counts after backgound subtraction, these entries can be removed
      # (these entries may have emerged when background is subtracted from small count numbers)
      # Removing zero counts is not done in CombinatorialProfiler
      if(removeZerosAfterBGsubt) {
         logindex_nocounts_bgsubt <- bgsubt$count==0
         if(sum(logindex_nocounts_bgsubt)>0) {
           cat("\n", sum(logindex_nocounts_bgsubt), "entries removed from count matrix due to zero counts after background subtraction:\n")
           print(bgsubt[logindex_nocounts_bgsubt, c("unique_sample_id", "bin", "sequence")][1:min(20,sum(logindex_nocounts_bgsubt)),]) # print max 20 entries
           cat("\n")
           bgsubt <- bgsubt[!logindex_nocounts_bgsubt,]
         }}
     
     # recalulate sample sum for bgsubt 
     bgsubt <- bgsubt %>%
       dplyr::group_by(unique_sample_id) %>%
       dplyr::mutate(sum = sum(count)) %>%
       dplyr::ungroup() 
    }
   
   
   
 ### run the following code separately for counts and bgsubt
  # counts and bgsubt have identical format and differ just in column "count"
  
  for(bg in bgloop) {
  
    if(bg=="bgsubt") {counts <- bgsubt} 
   
    plot_histo <- counts %>%
      filter(count > 0) %>%
      ggplot(aes(count)) +
      geom_histogram(bins=100) +
      facet_grid(helper_cat ~ bin) +
      scale_x_log10() +
      theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1), strip.text.y = element_text(angle=0))

    ggsave(filename=file.path(name_plotfolder, paste0("histogram_",e , "_", bg, ".png")),
           plot=plot_histo, 
           width = 200, height = 150, 
           units = c("mm"),  dpi = 600, device="png")
   
      ## cell distribution per variant normalised by bin countsum and cell fraction:
      # divide raw counts by sum counts for all sequences from one strain (barcode_fw) and  bin (barcode_rev)
      # and multiply by cell fraction
      # This normalized count (aka Cp) is used by CombinatorialProfiler
       counts$normalized_counts <- (counts$count / counts$sum) * counts$fraction
        
      # transformed bin index times normalised counts
       counts$fxCp <- counts$f * counts$normalized_counts 
      
      # translate nucleotide to codon with Biostrings # confimed with CombinatorialProfiler
      # no.init.codon=T because otherwise the alternative initiation codons CTG and TTG are 
      # translated to M instead to L when they are located at the first position of the hexamer.
      # alternative from GENETIC_CODE_TABLE: getGeneticCode("Alternative Yeast Nuclear", full.search=T)
      # The code "Alternative Yeast Nuclear" differs from GENETIC_CODE only in alt_init_codons
        counts$translation <- as.character(translate(DNAStringSet(as.character(counts$sequence)),
                                              genetic.code=GENETIC_CODE, no.init.codon=T))
      
      # produce output table
        counts <- data.frame(counts[,c(colnames(targets)[colnames(targets) %in% colnames(counts)], 
                                       "bin_rank", "f", "fxCp", "sequence", "sum", "translation", "count", "normalized_counts")])
        if(use_sub_experiment){
          counts <- counts[order(counts$experiment, counts$sub_experiment, counts$f, counts$sequence), ]
        } else {
          counts <- counts[order(counts$experiment, counts$f, counts$sequence), ]
        }
        
      
      ## calculate PSI (aka dsi) per hexamer (bynuc)
          listcats4index <- list(sequence=counts$sequence) # determine either 1 or 2 index categories for tapply (sequence, sub_experiment)
          if(use_sub_experiment) {
            listcats4index[["sub_experiment"]] <- counts$sub_experiment # use sub_experiment only if given
            }
          
          bynuc <- tapply(counts$fxCp, INDEX = listcats4index, sum) # calculate PSI numerator "fxCp_sum" 
          bynuc <- melt(bynuc) # for the case we have two INDEX variables
          colnames(bynuc) <- c(names(listcats4index), "fxCp_sum")
          bynuc$Cp_sum <- melt(tapply(counts$normalized_counts, INDEX = listcats4index, sum))[,"value"] # calculate PSI denominator "Cp_sum"
          bynuc$PSI <- bynuc$fxCp_sum / bynuc$Cp_sum # PSI
          
          bynuc$nonZeroCounts <- melt(tapply(counts$normalized_counts, INDEX = listcats4index, function(x) {sum(x>0, na.rm=T)}))[,"value"]
          
          # statistics
          statfunctions <- list("normalized_counts_min" = min, "normalized_counts_max" = max, "normalized_counts_mean" = mean, 
                             "normalized_counts_median" = median, "normalized_counts_std" = sd, "normalized_counts_sum" = sum)
          statfunctions_raw <- statfunctions
          names(statfunctions_raw) <- gsub("normalized_", "", names(statfunctions_raw))
          
          normcountstats <- sapply(statfunctions, function(x){ # same order as bynuc since same tapply
             melt(tapply(counts$normalized_counts, listcats4index, x, na.rm = TRUE))[,"value"]} 
             )
          rawcountstats <- sapply(statfunctions_raw, function(x){
            melt(tapply(counts$count, listcats4index, x, na.rm = TRUE))[,"value"]} 
            )
          
          bynuc <- data.frame(experiment=e, 
                              translation=as.character(translate(DNAStringSet(as.character(bynuc$sequence)), genetic.code=GENETIC_CODE, no.init.codon=T)),
                              bynuc[,!colnames(bynuc) %in% c("fxCp_sum", "Cp_sum")], 
                              normcountstats,
                              rawcountstats,
                              nfractions= melt(tapply(factor(counts$bin), listcats4index, dplyr::n_distinct))[,"value"])
          # NA entries in bynuc are removed later to keep conformity for byaa which calculated from bynuc and counts
          # Those byaa calculations based on bynuc contain the na.rm argument 
          
          
      ## calculate PSI per di-residue (byaa) from bynuc
          listcats4index_byaa <- list(translation=bynuc$translation)
          if(use_sub_experiment) {
            listcats4index_byaa[["sub_experiment"]] <- bynuc$sub_experiment # use sub_experiment only if given
          }
          
          byaa <- tapply(bynuc$PSI, listcats4index_byaa, median, na.rm=T)
          dimnames_aa_by_nuc <- dimnames(byaa) 
          byaa <- data.frame(rows=rownames(byaa), byaa, check.names=F, row.names = NULL)
          # transform to dataframe before melting because otherwise the di-residue "NA" is interpreted as NA
          byaa <- melt(byaa) 
          if(length(listcats4index_byaa)==1) {byaa <- byaa[,names(byaa) != "variable"]} # remove column variable=byaa
          colnames(byaa) <- c(names(listcats4index_byaa), "median_PSI")
          
          
        ## calculate pooled PSI per di-residue from counts
          listcats4index_byaa_pooled <- list(translation=counts$translation)
          if(use_sub_experiment) {
            listcats4index_byaa_pooled[["sub_experiment"]] <- counts$sub_experiment # use sub_experiment only if given
          }
          
          dimnames_aa_by_counts <- dimnames(tapply(counts$fxCp, listcats4index_byaa_pooled, sum)) 
          if(!identical(dimnames_aa_by_nuc, dimnames_aa_by_counts)) { # check for identical dim names 
            stop("\ndim names of aa matrix differ when calculated by count matrix and by nuc matrix!")
          } # if this gives a problem, create a separate byaa matrix from counts and merge to byaa matrix from nuc
 
          byaa$pooled_fxCp_sum <- melt(tapply(counts$fxCp, listcats4index_byaa_pooled, sum))[,"value"]
          byaa$pooled_Cp_sum <- melt(tapply(counts$normalized_counts, listcats4index_byaa_pooled, sum))[,"value"]
          byaa$pooled_PSI <- byaa$pooled_fxCp_sum / byaa$pooled_Cp_sum
          
          byaa_normcountstats <- sapply(statfunctions, function(x){
            melt(tapply(counts$normalized_counts, listcats4index_byaa_pooled, x, na.rm = TRUE))[,"value"]} 
          )
          byaa_rawcountstats <- sapply(statfunctions_raw, function(x){
            melt(tapply(counts$count, listcats4index_byaa_pooled, x, na.rm = TRUE))[,"value"]} 
          )
          
          # create output table
          byaa <- data.frame(experiment=e, 
                             byaa[,!colnames(byaa) %in% c("pooled_fxCp_sum", "pooled_Cp_sum")], 
                             byaa_normcountstats,
                             byaa_rawcountstats,
                             nfractions= melt(tapply(factor(counts$bin), listcats4index_byaa_pooled, dplyr::n_distinct))[,"value"],
                             nsequences= melt(tapply(bynuc$PSI, listcats4index_byaa, function(x){na.omit(length(x))}))[,"value"]
                             )

          # remove NA entries
          bynuc <- bynuc[!is.na(bynuc$PSI),]
          byaa <- byaa[!is.na(byaa$pooled_PSI),]
          
          # store results in list
          counts_all[[bg]][[e]] <- counts
          bynuc_all[[bg]][[e]] <- bynuc
          byaa_all[[bg]][[e]] <- byaa
          
          write.table(counts, file= file.path(out, paste0(e, "_", bg, "_counts.txt")), sep="\t", row.names = F, quote = F)
          write.table(bynuc, file= file.path(out, paste0(e, "_", bg, "_PSI_bynuc.txt")), sep="\t", row.names = F, quote = F)
          write.table(byaa, file= file.path(out, paste0(e, "_", bg, "_PSI_byaa.txt")), sep="\t", row.names = F, quote = F)
          
  } # end bg loop      
} # end e loop


for(bg in names(counts_all)) {
  
  if(length(counts_all[[bg]]) >1) {
  # rbind output tables
  counts_all[[bg]][["all_exp"]] <- rbind.fill(counts_all[[bg]])
  bynuc_all[[bg]][["all_exp"]] <- rbind.fill(bynuc_all[[bg]])
  byaa_all[[bg]][["all_exp"]] <- rbind.fill(byaa_all[[bg]])

  write.table(counts_all[[bg]][["all_exp"]], file= file.path(out, paste0(bg, "_counts_all.txt")), sep="\t", row.names = F, quote = F)
  write.table(bynuc_all[[bg]][["all_exp"]], file= file.path(out, paste0(bg, "_PSI_bynuc_all.txt")), sep="\t", row.names = F, quote = F)
  write.table(byaa_all[[bg]][["all_exp"]], file= file.path(out, paste0(bg, "_PSI_byaa_all.txt")), sep="\t", row.names = F, quote = F)

  }
}
############################################################################



#save the sessionInformation
writeLines(capture.output(sessionInfo()),paste(out, "/MPSprofiling_session_info.txt", sep=""))
save(counts_all, bynuc_all, byaa_all, file=paste0(out,"/MPSprofiling.RData"))











